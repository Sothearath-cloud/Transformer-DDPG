{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "import math # cos() for Rastrigin\n",
    "import copy # array-copying convenience\n",
    "import sys # max float\n",
    "import sympy as sp\n",
    "from sympy.plotting import plot\n",
    "from sympy.plotting import plot3d\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import deque\n",
    "from ddpg_agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameter/ constant\n",
    "bandwidth = 1e6  # [hz]\n",
    "Speed_of_light = 3e8  # [m/s]\n",
    "Carrier_freq = 2e9  # [hz]=1/s\n",
    "GBS_location = [0,0,20] #[m] 20\n",
    "Num_GBS = 1 \n",
    "NUM_User = 10\n",
    "NUM_UAV = 2\n",
    "Cell_Radius = 1000 #[m]\n",
    "UAV_Height = 100 #[m] \"Cell-Edge User Offloading via Flying UAV in Non-Uniform Heterogeneous Cellular Networks,\" in IEEE Transactions on Wireless Communications, vol. 19, no. 4, pp. 2411-2426, April 2020, doi: 10.1109/TWC.2020.2964656.\n",
    "P_UAV = 10 **(37 / 10) / 1000 #[watt] \n",
    "P_GBS = 10 **(40 / 10) / 1000 #[watt]\n",
    "P_MAX = 0.4 # 26dbm, user\n",
    "# P_MIN = 0.1 # 20 dbm\n",
    "epsilon = 0.38 # power amplifier efficiency\n",
    "AWGN_DBM = -174 #[dBm]\n",
    "AWGN_W = 10 ** (AWGN_DBM / 10) / 1000\n",
    "air_density = 1.225 # Air density at sea level (kg/m³)\n",
    "g = 9.81            # Gravitational acceleration (m/s²)\n",
    "mass = 0.5           # Mass of UAV in kg\n",
    "radius = 0.2         # Rotor radius in meters\n",
    "num_rotors = 4\n",
    "P_MOVE =10 #[watt]\n",
    "V = 20 # m/s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-User Data Generation with Poisson Distribution\n",
    "def generate_multi_user_data(num_users=5, base_lambda=10, num_hours=3*720, start_date=None, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic hourly traffic data for multiple users using Poisson distribution.\n",
    "\n",
    "    Args:\n",
    "        num_users: Number of users to generate data for\n",
    "        base_lambda: Base lambda for Poisson distribution (Mbps avg Rate)\n",
    "        num_hours: Number of hours to generate\n",
    "        start_date: Starting date for the data\n",
    "        seed: Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        df: DataFrame with timestamps and traffic values for all users\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    days_in_month = 2 * 30  # Length of the monthly pattern\n",
    "\n",
    "    # Common daily pattern - all users follow similar daily rhythms (low at night, high in evening)\n",
    "    base_daily_pattern = np.array([\n",
    "        0.05, 0.05, 0.05, 0.05, 0.05, 0.05,  # Midnight to 6 AM (low traffic)\n",
    "        0.10, 0.20, 0.35, 0.50, 0.65, 0.80,  # Morning to afternoon (moderate traffic)\n",
    "        0.90, 0.98, 1.00, 0.98, 0.90, 0.80,  # Evening peak hours (high traffic)\n",
    "        0.65, 0.50, 0.35, 0.20, 0.10, 0.05   # Late night (declining traffic)\n",
    "    ])\n",
    "\n",
    "    # Base weekly pattern - all users follow similar weekly patterns\n",
    "    base_weekly_pattern = np.array([0.9, 0.7, 0.7, 0.6, 0.8, 1.0, 1.0])  # Mon-Sun\n",
    "\n",
    "    # Base monthly pattern - common seasonal effect\n",
    "    base_monthly_pattern = 1 - 0.2 * np.sin(2 * np.pi * np.arange(days_in_month) / days_in_month)\n",
    "\n",
    "    # Set the start date to 2 months ago from today if not provided\n",
    "    if start_date is None:\n",
    "        start_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=60)\n",
    "\n",
    "    # Generate timestamps (same for all users)\n",
    "    timestamps = []\n",
    "    current_time = start_date\n",
    "    for _ in range(num_hours):\n",
    "        timestamps.append(current_time)\n",
    "        current_time += timedelta(hours=1)\n",
    "\n",
    "    # Create user-specific patterns with variations\n",
    "    user_data = {}\n",
    "    user_patterns = {}\n",
    "\n",
    "    for user_id in range(1, num_users + 1):\n",
    "        # Create user-specific variations of the patterns\n",
    "        user_lambda = base_lambda * random.uniform(0.5, 2.0)  # Different baseline traffic volume\n",
    "\n",
    "        # Daily pattern variation (shift peak hours slightly, change intensities)\n",
    "        shift = random.randint(-2, 2)  # Shift peak by -2 to +2 hours\n",
    "        daily_pattern = np.roll(base_daily_pattern, shift)\n",
    "        daily_pattern = daily_pattern * random.uniform(0.8, 1.2)  # Vary intensity\n",
    "        daily_pattern = np.clip(daily_pattern, 0.01, 1.0)  # Ensure values are reasonable\n",
    "\n",
    "        # Weekly pattern variation (some users more active on weekends, others on weekdays)\n",
    "        weekly_variation = np.random.uniform(0.8, 1.2, size=7)\n",
    "        weekly_pattern = base_weekly_pattern * weekly_variation\n",
    "        weekly_pattern = np.clip(weekly_pattern, 0.3, 1.5)  # Ensure values are reasonable\n",
    "\n",
    "        # Monthly pattern might have different phases for different users\n",
    "        phase_shift = random.uniform(0, 2*np.pi)\n",
    "        monthly_pattern = 1 - 0.2 * np.sin(2 * np.pi * np.arange(days_in_month) / days_in_month + phase_shift)\n",
    "\n",
    "        # Store patterns for reference\n",
    "        user_patterns[user_id] = {\n",
    "            \"lambda\": user_lambda,\n",
    "            \"daily\": daily_pattern,\n",
    "            \"weekly\": weekly_pattern,\n",
    "            \"monthly\": monthly_pattern\n",
    "        }\n",
    "\n",
    "        # Generate traffic values for this user\n",
    "        values = []\n",
    "        for t in timestamps:\n",
    "            hour = t.hour\n",
    "            day_of_week = t.weekday()\n",
    "            day_of_month = (t - start_date).days % days_in_month\n",
    "\n",
    "            # Combine patterns to adjust lambda\n",
    "            adjusted_lambda = (\n",
    "                user_lambda *\n",
    "                daily_pattern[hour] *\n",
    "                weekly_pattern[day_of_week] *\n",
    "                monthly_pattern[day_of_month]\n",
    "            )\n",
    "\n",
    "            # Add small random trend component\n",
    "            trend_factor = 1.0 + 0.0005 * (t - start_date).days  # Slight growth over time\n",
    "            adjusted_lambda *= trend_factor\n",
    "\n",
    "            # Generate traffic using Poisson distribution\n",
    "            value = np.random.poisson(adjusted_lambda)\n",
    "            values.append(value)\n",
    "\n",
    "        user_data[user_id] = values\n",
    "\n",
    "    # Create DataFrame with all users\n",
    "    df_data = {\"timestamp\": timestamps}\n",
    "    for user_id, values in user_data.items():\n",
    "        df_data[f\"user_{user_id}\"] = values\n",
    "\n",
    "    df = pd.DataFrame(df_data)\n",
    "\n",
    "    return df, user_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, user_patterns = generate_multi_user_data(num_users=5, base_lambda=10, num_hours=3*720, start_date=None, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_1</th>\n",
       "      <th>user_2</th>\n",
       "      <th>user_3</th>\n",
       "      <th>user_4</th>\n",
       "      <th>user_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-21 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-06-21 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-06-21 02:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-06-21 03:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-06-21 04:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>2025-09-18 19:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>2025-09-18 20:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>2025-09-18 21:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>2025-09-18 22:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>2025-09-18 23:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2160 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               timestamp  user_1  user_2  user_3  user_4  user_5\n",
       "0    2025-06-21 00:00:00       1       0       0       0       2\n",
       "1    2025-06-21 01:00:00       1       2       0       1       1\n",
       "2    2025-06-21 02:00:00       2       0       0       1       0\n",
       "3    2025-06-21 03:00:00       0       1       0       1       0\n",
       "4    2025-06-21 04:00:00       0       0       1       1       0\n",
       "...                  ...     ...     ...     ...     ...     ...\n",
       "2155 2025-09-18 19:00:00       4       2       2       0       3\n",
       "2156 2025-09-18 20:00:00       0       0       2       1       2\n",
       "2157 2025-09-18 21:00:00       2       0       0       4       6\n",
       "2158 2025-09-18 22:00:00       0       1       2       1       7\n",
       "2159 2025-09-18 23:00:00       1       0       0       0       2\n",
       "\n",
       "[2160 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding for Transformer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=10):\n",
    "        \"\"\"Initialize positional encoding.\"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        # Apply sin/cos functions\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add batch dimension and register as buffer (not a parameter)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Add positional encoding to input.\"\"\"\n",
    "        return x + self.pe[:, :x.size(1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention Module\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"Initialize multi-head attention.\"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, d_k).\"\"\"\n",
    "        # x shape: [batch_size, seq_len, d_model]\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.d_k)\n",
    "        # Transpose to [batch_size, num_heads, seq_len, d_k]\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"Forward pass for multi-head attention.\"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Linear projections\n",
    "        q = self.W_q(query)  # [batch_size, seq_len_q, d_model]\n",
    "        k = self.W_k(key)    # [batch_size, seq_len_k, d_model]\n",
    "        v = self.W_v(value)  # [batch_size, seq_len_v, d_model]\n",
    "\n",
    "        # Split heads\n",
    "        q = self.split_heads(q, batch_size)  # [batch_size, num_heads, seq_len_q, d_k]\n",
    "        k = self.split_heads(k, batch_size)  # [batch_size, num_heads, seq_len_k, d_k]\n",
    "        v = self.split_heads(v, batch_size)  # [batch_size, num_heads, seq_len_v, d_k]\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        # matmul: [batch_size, num_heads, seq_len_q, seq_len_k]\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Apply softmax\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # Apply attention weights to values\n",
    "        # context: [batch_size, num_heads, seq_len_q, d_k]\n",
    "        context = torch.matmul(attention_weights, v)\n",
    "\n",
    "        # Transpose and concatenate heads\n",
    "        # context: [batch_size, seq_len_q, num_heads, d_k]\n",
    "        context = context.transpose(1, 2).contiguous()\n",
    "\n",
    "        # context: [batch_size, seq_len_q, d_model]\n",
    "        context = context.view(batch_size, -1, self.d_model)\n",
    "\n",
    "        # Final linear projection\n",
    "        output = self.W_o(context)  # [batch_size, seq_len_q, d_model]\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encoder Block\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"Initialize the encoder block.\"\"\"\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        # Multi-head attention\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # Feed-forward network\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "        # Layer normalization and dropout\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"Forward pass for encoder block.\"\"\"\n",
    "        # Self-attention with residual connection and layer norm\n",
    "        attn_output = self.self_attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # Feed-forward network with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Transformer Encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads, d_ff, num_layers, dropout=0.1, max_seq_len=1000):\n",
    "        \"\"\"Initialize the transformer encoder.\"\"\"\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        # Input projection\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Stack of encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"Forward pass for transformer encoder.\"\"\"\n",
    "        # Project input to d_model dimensions\n",
    "        x = self.input_projection(x)  # [batch_size, seq_len, d_model]\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Pass through encoder layers\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x, mask)\n",
    "\n",
    "        return x  # [batch_size, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralized Multi-User Traffic Forecasting Model\n",
    "class CentralizedTrafficForecastingModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_users,\n",
    "                 input_dim=1,\n",
    "                 d_model=128,\n",
    "                 num_heads=8,\n",
    "                 d_ff=256,\n",
    "                 num_layers=4,\n",
    "                 dropout=0.1,\n",
    "                 pred_horizon=24):\n",
    "        \"\"\"\n",
    "        Initialize centralized traffic forecasting model for multiple users.\n",
    "\n",
    "        Args:\n",
    "            num_users: Number of users to predict for\n",
    "            input_dim: Input dimension per user (usually 1 for traffic data)\n",
    "            d_model: Hidden dimension of the model\n",
    "            num_heads: Number of attention heads\n",
    "            d_ff: Feed-forward dimension\n",
    "            num_layers: Number of encoder layers\n",
    "            dropout: Dropout rate\n",
    "            pred_horizon: Number of time steps to predict (24 for a day)\n",
    "        \"\"\"\n",
    "        super(CentralizedTrafficForecastingModel, self).__init__()\n",
    "\n",
    "        # Store number of users\n",
    "        self.num_users = num_users\n",
    "\n",
    "        # Store input dimensions\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # Transformer encoder - takes multi-user input\n",
    "        self.encoder = TransformerEncoder(\n",
    "            input_dim=input_dim * num_users,  # Input has data for all users\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            d_ff=d_ff,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Global average pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1) # average value (a,b,c) ==> (a,b,1)\n",
    "\n",
    "        # Output projection - produces predictions for all users\n",
    "        self.output_layer = nn.Linear(d_model, pred_horizon * num_users)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for centralized traffic forecasting model.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, seq_len, input_dim * num_users]\n",
    "               where each feature dimension contains data for one user\n",
    "\n",
    "        Returns:\n",
    "            Predictions for all users of shape [batch_size, pred_horizon * num_users]\n",
    "        \"\"\"\n",
    "        # Run through encoder\n",
    "        enc_output = self.encoder(x)  # [batch_size, seq_len, d_model]\n",
    "\n",
    "        # Global average pooling\n",
    "        pooling_input = enc_output.transpose(1, 2)  # [batch_size, d_model, seq_len]\n",
    "        avg_representation = self.global_avg_pool(pooling_input).squeeze(-1)  # [batch_size, d_model]\n",
    "\n",
    "        # Final output projection - produces predictions for all users\n",
    "        output = self.output_layer(avg_representation)  # [batch_size, pred_horizon * num_users]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for multi-user time series forecasting\n",
    "class MultiUserTrafficDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"Initialize dataset for multi-user data.\"\"\"\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_arrays = np.array([\n",
    "   [[0.01], [0.2], [0.01], [0.6], [0.01], [0.8], [0.4], [0.01], [1.0]],  # user_1's transformed data\n",
    "   [[0.1], [0.4], [0.3], [0.0], [0.2], [0.35], [0.05], [0.25], [0.15]],   # user_2's transformed data\n",
    "   [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]   ] )    # user_3's zero array\n",
    "input_arrays = np.stack(input_arrays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = input_arrays.reshape(1, 27, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processor for Multi-User Traffic Forecasting\n",
    "class CentralizedDataProcessor:\n",
    "    def __init__(self, df, user_cols, timestamp_col='timestamp'):\n",
    "        \"\"\"\n",
    "        Initialize data processor for multiple users.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame with traffic data\n",
    "            user_cols: List of column names for user traffic data\n",
    "            timestamp_col: Name of the timestamp column\n",
    "        \"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.user_cols = user_cols\n",
    "        self.timestamp_col = timestamp_col\n",
    "        self.num_users = len(user_cols)\n",
    "\n",
    "        # Initialize a scaler for each user\n",
    "        self.scalers = {}\n",
    "        for user_col in user_cols:\n",
    "            self.scalers[user_col] = MinMaxScaler(feature_range=(0, 1))\n",
    "            # Fit scaler on this user's traffic data\n",
    "            user_data = self.df[user_col].values.reshape(-1, 1)\n",
    "            if len(user_data) > 0:  # Check if data is not empty\n",
    "                self.scalers[user_col].fit(user_data)\n",
    "\n",
    "        # Ensure timestamp is datetime\n",
    "        if not pd.api.types.is_datetime64_any_dtype(self.df[timestamp_col]):\n",
    "            self.df[timestamp_col] = pd.to_datetime(self.df[timestamp_col])\n",
    "\n",
    "        # Sort by timestamp\n",
    "        self.df = self.df.sort_values(by=timestamp_col)\n",
    "\n",
    "    def prepare_input_for_date(self, target_date, seq_length=120):\n",
    "        \"\"\"\n",
    "        Prepare input data for predicting traffic on a specific date for all users.\n",
    "\n",
    "        Args:\n",
    "            target_date: The date to predict (datetime or string)\n",
    "            seq_length: Fixed sequence length for model input\n",
    "\n",
    "        Returns:\n",
    "            model_input: Normalized input tensor for the model\n",
    "            target_hours: Hours to be predicted\n",
    "        \"\"\"\n",
    "        if isinstance(target_date, str):\n",
    "            target_date = pd.to_datetime(target_date)\n",
    "\n",
    "        # Get data from previous 3 days (72 hours)\n",
    "        three_days_ago = target_date - timedelta(days=3)\n",
    "        prev_3_days = self.df[(self.df[self.timestamp_col] >= three_days_ago) &\n",
    "                              (self.df[self.timestamp_col] < target_date)]\n",
    "\n",
    "        # Get data from same day of the week for the past 2 weeks\n",
    "        day_of_week = target_date.weekday()\n",
    "\n",
    "        # First previous week\n",
    "        week_ago = target_date - timedelta(days=7)\n",
    "        week_ago_day = week_ago.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        prev_week_same_day = self.df[(self.df[self.timestamp_col] >= week_ago_day) &\n",
    "                                     (self.df[self.timestamp_col] < week_ago_day + timedelta(days=1))]\n",
    "\n",
    "        # Second previous week\n",
    "        two_weeks_ago = target_date - timedelta(days=14)\n",
    "        two_weeks_ago_day = two_weeks_ago.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        prev_2_weeks_same_day = self.df[(self.df[self.timestamp_col] >= two_weeks_ago_day) &\n",
    "                                        (self.df[self.timestamp_col] < two_weeks_ago_day + timedelta(days=1))]\n",
    "\n",
    "        # Combine all data\n",
    "        input_data_frames = [prev_3_days, prev_week_same_day, prev_2_weeks_same_day]\n",
    "        all_input_df = pd.concat(input_data_frames)\n",
    "        all_input_df = all_input_df.sort_values(by=self.timestamp_col)\n",
    "\n",
    "        if all_input_df.empty:\n",
    "            raise ValueError(f\"No input data available for {target_date}\")\n",
    "\n",
    "        # Prepare multi-user input\n",
    "        # Scale data for each user and concatenate horizontally\n",
    "        input_arrays = []\n",
    "        for user_col in self.user_cols:\n",
    "            # Skip if no data for this user\n",
    "            if user_col in all_input_df.columns:\n",
    "                # Replace zeros with a small non-zero value (e.g., 0.01)\n",
    "                all_input_df[user_col] = all_input_df[user_col].replace(0, 0.01)\n",
    "            if user_col not in all_input_df.columns:\n",
    "                # Create array of zeros for this user\n",
    "                user_array = np.zeros((len(all_input_df), 1)) # (row, column)\n",
    "            else:\n",
    "                # Scale this user's data\n",
    "                try:\n",
    "                    user_data = all_input_df[user_col].values.reshape(-1, 1)\n",
    "                    if len(user_data) > 0:\n",
    "                        user_array = self.scalers[user_col].transform(user_data)\n",
    "                    else:\n",
    "                        user_array = np.zeros((len(all_input_df), 1))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error scaling data for {user_col}: {e}\")\n",
    "                    user_array = np.zeros((len(all_input_df), 1))\n",
    "\n",
    "            input_arrays.append(user_array)\n",
    "        # Concatenate all user data horizontally\n",
    "        if input_arrays:\n",
    "            input_values = np.hstack(input_arrays)\n",
    "        else:\n",
    "            # If no user data available, create zero array\n",
    "            input_values = np.zeros((len(all_input_df), self.num_users))\n",
    "\n",
    "        # Handle sequence length consistency\n",
    "        actual_length = input_values.shape[0] #sequence leangth\n",
    "        if actual_length > seq_length:\n",
    "            # If we have too much data, take the most recent seq_length values\n",
    "            input_values = input_values[-seq_length:]\n",
    "        elif actual_length < seq_length:\n",
    "            # If we don't have enough data, pad with zeros at the beginning (focus on current data)\n",
    "            padding = np.zeros((seq_length - actual_length, input_values.shape[1]))\n",
    "            input_values = np.vstack((padding, input_values))\n",
    "\n",
    "        # Get target hours for prediction (24 hours starting from target_date)\n",
    "        target_hours = [target_date + timedelta(hours=i) for i in range(24)]\n",
    "\n",
    "        # before input_values [seq_len, features=num_users]\n",
    "        # Reshape to model input format: [batch_size=1, seq_len, features=num_users]\n",
    "        model_input = input_values.reshape(1, seq_length, -1) #add one more dimension of batch_size=1, to match the input format\n",
    "        return model_input, target_hours\n",
    "\n",
    "    def prepare_training_data(self, start_date, end_date, seq_length=120, pred_horizon=24):\n",
    "        \"\"\"\n",
    "        Prepare training data for all users in a date range.\n",
    "\n",
    "        Args:\n",
    "            start_date: Start of training period\n",
    "            end_date: End of training period\n",
    "            seq_length: Input sequence length\n",
    "            pred_horizon: Prediction horizon (usually 24 hours)\n",
    "\n",
    "        Returns:\n",
    "            X_train: Training inputs for all users combined\n",
    "            y_train: Training targets for all users combined\n",
    "        \"\"\"\n",
    "        if isinstance(start_date, str):\n",
    "            start_date = pd.to_datetime(start_date)\n",
    "        if isinstance(end_date, str):\n",
    "            end_date = pd.to_datetime(end_date)\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        # Process each day in the date range\n",
    "        current_date = start_date\n",
    "        while current_date <= end_date:\n",
    "            try:\n",
    "                # Get input data for this date (all users)\n",
    "                model_input, _ = self.prepare_input_for_date(current_date, seq_length)\n",
    "\n",
    "                # Get target data (next 24 hours from current_date) for all users\n",
    "                target_start = current_date\n",
    "                target_end = target_start + timedelta(days=1)\n",
    "                target_data = self.df[(self.df[self.timestamp_col] >= target_start) &\n",
    "                                     (self.df[self.timestamp_col] < target_end)]\n",
    "\n",
    "                # Only use days with complete target data (24 hours)\n",
    "                if len(target_data) == pred_horizon:\n",
    "                    # Scale target data for each user and concatenate\n",
    "                    target_arrays = []\n",
    "                    for user_col in self.user_cols:\n",
    "                        if user_col in target_data.columns:\n",
    "                            user_data = target_data[user_col].values.reshape(-1, 1)\n",
    "                            if len(user_data) > 0:\n",
    "                                user_array = self.scalers[user_col].transform(user_data)\n",
    "                                target_arrays.append(user_array.flatten())\n",
    "                            else:\n",
    "                                target_arrays.append(np.zeros(pred_horizon))\n",
    "                        else:\n",
    "                            target_arrays.append(np.zeros(pred_horizon))\n",
    "\n",
    "                    # Concatenate all target arrays\n",
    "                    target_values = np.concatenate(target_arrays)\n",
    "                    # print(\"target_values\",target_values)\n",
    "\n",
    "                    # Add to training sets\n",
    "                    X.append(model_input[0])  # Shape: [seq_length, num_users]\n",
    "                    y.append(target_values)  # Shape: [pred_horizon * num_users]\n",
    "                else:\n",
    "                    print(f\"Skipping {current_date.date()} - incomplete target data ({len(target_data)} hours)\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error preparing data for {current_date.date()}: {e}\")\n",
    "\n",
    "            # Move to next day\n",
    "            current_date += timedelta(days=1)\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        if not X:\n",
    "            raise ValueError(\"No valid training samples were generated\")\n",
    "\n",
    "        # Convert to numpy arrays using stack to ensure consistent shapes\n",
    "        X = np.stack(X)\n",
    "        y = np.stack(y)\n",
    "\n",
    "        print(f\"Prepared {len(X)} training samples\")\n",
    "        print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def inverse_transform_predictions(self, predictions):\n",
    "        \"\"\"\n",
    "        Convert scaled predictions back to original scale for all users.\n",
    "\n",
    "        Args:\n",
    "            predictions: Scaled predictions from the model of shape [pred_horizon * num_users]\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of original scale predictions for each user\n",
    "        \"\"\"\n",
    "        pred_horizon = 148  # Assuming 24-hour prediction\n",
    "        user_predictions = {}\n",
    "\n",
    "        for i, user_col in enumerate(self.user_cols):\n",
    "            # Extract this user's predictions\n",
    "            user_start = i * pred_horizon\n",
    "            user_end = (i + 1) * pred_horizon\n",
    "            user_pred_scaled = predictions[user_start:user_end]\n",
    "            # Reshape for inverse transform\n",
    "            user_pred_scaled = user_pred_scaled.reshape(-1, 1)\n",
    "\n",
    "            # Inverse transform\n",
    "            user_pred = self.scalers[user_col].inverse_transform(user_pred_scaled).flatten()\n",
    "\n",
    "            # Store in dictionary\n",
    "            user_predictions[user_col] = user_pred\n",
    "        return user_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralized Traffic Forecaster\n",
    "class CentralizedTrafficForecaster:\n",
    "    def __init__(self, df, user_cols=None, timestamp_col='timestamp'):\n",
    "        \"\"\"\n",
    "        Initialize centralized traffic forecaster.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame with traffic data for all users\n",
    "            user_cols: List of column names for user traffic data (if None, auto-detect)\n",
    "            timestamp_col: Name of the timestamp column\n",
    "        \"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.timestamp_col = timestamp_col\n",
    "\n",
    "        # Auto-detect user columns if not provided\n",
    "        if user_cols is None:\n",
    "            # Assume all columns except timestamp are user data\n",
    "            self.user_cols = [col for col in df.columns if col != timestamp_col]\n",
    "        else:\n",
    "            self.user_cols = user_cols\n",
    "\n",
    "        self.num_users = len(self.user_cols)\n",
    "        print(f\"Detected {self.num_users} users: {self.user_cols}\")\n",
    "\n",
    "        # Initialize data processor\n",
    "        self.data_processor = CentralizedDataProcessor(\n",
    "            df, self.user_cols, timestamp_col\n",
    "        )\n",
    "\n",
    "        # Initialize model\n",
    "        self.model = CentralizedTrafficForecastingModel(\n",
    "            num_users=self.num_users,\n",
    "            input_dim=1,\n",
    "            d_model=128,\n",
    "            num_heads=8,\n",
    "            d_ff=256,\n",
    "            num_layers=4,\n",
    "            dropout=0.1,\n",
    "            pred_horizon= 24\n",
    "        ).to(device)\n",
    "\n",
    "        # Initialize optimizer and loss function\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.00001)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def train(self, start_date, end_date, batch_size=32, epochs=50):\n",
    "        \"\"\"\n",
    "        Train the centralized model.\n",
    "\n",
    "        Args:\n",
    "            start_date: Start of training period\n",
    "            end_date: End of training period\n",
    "            batch_size: Batch size for training\n",
    "            epochs: Number of epochs to train\n",
    "\n",
    "        Returns:\n",
    "            Training loss history\n",
    "        \"\"\"\n",
    "        print(f\"Training centralized model for {self.num_users} users...\")\n",
    "\n",
    "        # Prepare training data\n",
    "        try:\n",
    "            X_train, y_train = self.data_processor.prepare_training_data(\n",
    "                start_date, end_date, seq_length=120, pred_horizon= 24\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing training data: {e}\")\n",
    "            return []\n",
    "\n",
    "        # Create dataset and dataloader\n",
    "        train_dataset = MultiUserTrafficDataset(X_train, y_train) # convert data format to tensor\n",
    "        print(\"train_dataset\",train_dataset)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        print(\"train_loader\",train_loader)\n",
    "\n",
    "        # Training history\n",
    "        train_losses = []\n",
    "\n",
    "        # Training loop\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                # Move data to device\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                # Zero the gradients\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self.model(X_batch)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = self.criterion(outputs, y_batch)\n",
    "\n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            # Calculate average loss for the epoch\n",
    "            avg_loss = epoch_loss / len(train_loader) if len(train_loader) > 0 else float('inf')\n",
    "            train_losses.append(avg_loss)\n",
    "\n",
    "            # Print progress\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}')\n",
    "\n",
    "        return train_losses\n",
    "\n",
    "    def predict(self, target_date):\n",
    "        \"\"\"\n",
    "        Predict traffic for all users on a specific date.\n",
    "\n",
    "        Args:\n",
    "            target_date: The date to predict\n",
    "\n",
    "        Returns:\n",
    "            predictions: Dictionary of traffic predictions for each user\n",
    "            target_hours: Hours corresponding to predictions\n",
    "        \"\"\"\n",
    "        # Prepare input data\n",
    "        model_input, target_hours = self.data_processor.prepare_input_for_date(target_date)\n",
    "\n",
    "        # Convert to torch tensor\n",
    "        model_input = torch.tensor(model_input, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            predictions_scaled = self.model(model_input).cpu().numpy()\n",
    "            print(\"predictions_scaled\",predictions_scaled)\n",
    "\n",
    "        # Convert back to original scale and split by user\n",
    "        user_predictions = self.data_processor.inverse_transform_predictions(predictions_scaled[0])\n",
    "        return user_predictions, target_hours # data users in each hour\n",
    "\n",
    "    def plot_predictions(self, target_date):\n",
    "        \"\"\"\n",
    "        Plot predictions for all users on a specific date.\n",
    "\n",
    "        Args:\n",
    "            target_date: The date to predict\n",
    "\n",
    "        Returns:\n",
    "            fig: Matplotlib figure\n",
    "        \"\"\"\n",
    "        # Get predictions\n",
    "        user_predictions, target_hours = self.predict(target_date)\n",
    "\n",
    "        \n",
    "\n",
    "        # Get actual data\n",
    "        actual_data = {}\n",
    "        try:\n",
    "            actual_df = self.df[(self.df[self.timestamp_col] >= target_date) &\n",
    "                              (self.df[self.timestamp_col] < target_date + timedelta(days=1))]\n",
    "\n",
    "            for user_col in self.user_cols:\n",
    "                if user_col in actual_df.columns and len(actual_df) == 24:\n",
    "                    actual_data[user_col] = actual_df[user_col].values\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting actual data: {e}\")\n",
    "        \n",
    "        \n",
    "        # Create plot with subplots for each user\n",
    "        fig, axes = plt.subplots(len(self.user_cols), 1, figsize=(15, 5 * len(self.user_cols)))\n",
    "\n",
    "        # Handle case with only one user\n",
    "        if len(self.user_cols) == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        # Plot each user's predictions\n",
    "        for i, user_col in enumerate(self.user_cols):\n",
    "            ax = axes[i]\n",
    "\n",
    "            # Plot predictions\n",
    "            ax.plot(target_hours, user_predictions[user_col], 'b-', label='Prediction')\n",
    "\n",
    "            # Plot actual data if available\n",
    "            if user_col in actual_data:\n",
    "                ax.plot(target_hours, actual_data[user_col], 'r-', label='Actual')\n",
    "\n",
    "            # Format subplot\n",
    "            ax.set_title(f'Traffic Forecast for {user_col} on {pd.to_datetime(target_date).strftime(\"%Y-%m-%d\")}')\n",
    "            if i == len(self.user_cols) - 1:\n",
    "                ax.set_xlabel('Hour')\n",
    "            ax.set_ylabel('Traffic (Mbps)')\n",
    "            ax.grid(True)\n",
    "            ax.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def evaluate(self, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Evaluate model performance over a date range.\n",
    "\n",
    "        Args:\n",
    "            start_date: Start of evaluation period\n",
    "            end_date: End of evaluation period\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of evaluation metrics for each user\n",
    "        \"\"\"\n",
    "        # Initialize metrics\n",
    "        user_metrics = {user_col: {'rmse': [], 'mae': [], 'mape': []} for user_col in self.user_cols}\n",
    "        dates = []\n",
    "\n",
    "        # Evaluate each day in the range\n",
    "        current_date = pd.to_datetime(start_date)\n",
    "        end_date = pd.to_datetime(end_date)\n",
    "\n",
    "        while current_date <= end_date:\n",
    "            try:\n",
    "                # Get predictions for all users\n",
    "                user_predictions, _ = self.predict(current_date)\n",
    "\n",
    "                # Get actual data\n",
    "                actual_df = self.df[(self.df[self.timestamp_col] >= current_date) &\n",
    "                                   (self.df[self.timestamp_col] < current_date + timedelta(days=1))]\n",
    "\n",
    "                # Only evaluate if we have complete actual data\n",
    "                if len(actual_df) == 148:\n",
    "                    dates.append(current_date)\n",
    "\n",
    "                    # Calculate metrics for each user\n",
    "                    for user_col in self.user_cols:\n",
    "                        if user_col in actual_df.columns:\n",
    "                            actual_data = actual_df[user_col].values\n",
    "                            predictions = user_predictions[user_col]\n",
    "\n",
    "                            # Calculate metrics\n",
    "                            rmse = np.sqrt(np.mean((predictions - actual_data) ** 2)) # ex: error mean([0.1, 0.2, 0.4]) \n",
    "                            mae = np.mean(np.abs(predictions - actual_data))\n",
    "                            # Add small epsilon to avoid division by zero\n",
    "                            mape = np.mean(np.abs((actual_data - predictions) / (actual_data + 1e-5))) * 100\n",
    "\n",
    "                            # Store metrics\n",
    "                            user_metrics[user_col]['rmse'].append(rmse)\n",
    "                            user_metrics[user_col]['mae'].append(mae)\n",
    "                            user_metrics[user_col]['mape'].append(mape)\n",
    "\n",
    "                            print(f\"Evaluated {user_col} on {current_date.date()}: RMSE={rmse:.2f}, MAE={mae:.2f}, MAPE={mape:.2f}%\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating on {current_date.date()}: {e}\")\n",
    "\n",
    "            # Move to next day\n",
    "            print(\"current_date\",current_date)\n",
    "            current_date += timedelta(days=1)\n",
    "\n",
    "        # Calculate average metrics for each user\n",
    "        avg_metrics = {}\n",
    "        for user_col, metrics in user_metrics.items():\n",
    "            avg_metrics[user_col] = {\n",
    "                'avg_rmse': np.mean(metrics['rmse']) if metrics['rmse'] else None,\n",
    "                'avg_mae': np.mean(metrics['mae']) if metrics['mae'] else None,\n",
    "                'avg_mape': np.mean(metrics['mape']) if metrics['mape'] else None\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            'daily_metrics': dict(zip([d.date() for d in dates], user_metrics)),\n",
    "            'avg_metrics': avg_metrics\n",
    "        }\n",
    "\n",
    "    def plot_evaluation_metrics(self, evaluation_results):\n",
    "        \"\"\"\n",
    "        Plot average evaluation metrics for all users.\n",
    "\n",
    "        Args:\n",
    "            evaluation_results: Results from evaluate()\n",
    "\n",
    "        Returns:\n",
    "            fig: Matplotlib figure\n",
    "        \"\"\"\n",
    "        if 'avg_metrics' not in evaluation_results:\n",
    "            print(\"No average metrics available in evaluation results\")\n",
    "            return None\n",
    "\n",
    "        # Extract metrics\n",
    "        avg_metrics = evaluation_results['avg_metrics']\n",
    "\n",
    "        # Create figure with subplots for different metrics\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        metrics = ['avg_rmse', 'avg_mae', 'avg_mape']\n",
    "        titles = ['Average RMSE', 'Average MAE', 'Average MAPE (%)']\n",
    "\n",
    "        # For each metric type\n",
    "        for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "            ax = axes[i]\n",
    "\n",
    "            # Extract values for this metric\n",
    "            user_names = []\n",
    "            metric_values = []\n",
    "\n",
    "            for user_col, user_avg_metrics in avg_metrics.items():\n",
    "                if user_avg_metrics[metric] is not None:\n",
    "                    user_names.append(user_col)\n",
    "                    metric_values.append(user_avg_metrics[metric])\n",
    "\n",
    "            # Create bar chart\n",
    "            if metric_values:\n",
    "                ax.bar(user_names, metric_values)\n",
    "                ax.set_title(title)\n",
    "                ax.set_ylabel(title.split()[1])\n",
    "                ax.set_xlabel('User')\n",
    "                ax.grid(True, axis='y')\n",
    "\n",
    "                # Rotate x-labels if there are many users\n",
    "                if len(user_names) > 5:\n",
    "                    ax.set_xticklabels(user_names, rotation=45, ha='right')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the model to disk.\"\"\"\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict()\n",
    "        }, filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load the model from disk.\"\"\"\n",
    "        checkpoint = torch.load(filepath)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(f\"Model loaded from {filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with synthetic data\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Generating synthetic traffic data for multiple users...\")\n",
    "\n",
    "    # Generate data for 5 users over 3 months\n",
    "    num_users = 5\n",
    "    start_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=60)\n",
    "    df, user_patterns = generate_multi_user_data(\n",
    "        num_users=num_users,\n",
    "        base_lambda=15,  # 10 Base traffic rate\n",
    "        num_hours=3*720,  # 3 months of hourly data\n",
    "        start_date=start_date,\n",
    "        seed=42  # For reproducibility\n",
    "    )\n",
    "\n",
    "    # Save generated data to CSV\n",
    "    df.to_csv('multi_user_traffic_data.csv', index=False)\n",
    "    print(f\"Saved synthetic data to multi_user_traffic_data.csv\")\n",
    "\n",
    "    # Plot sample of the generated data for each user\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for i in range(1, num_users + 1):\n",
    "        user_col = f\"user_{i}\"\n",
    "        plt.plot(df['timestamp'][:720], df[user_col][:720], label=user_col)  # Plot first month\n",
    "\n",
    "    plt.title('Generated Traffic Data for Multiple Users (First Month)')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Traffic (Mbps)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('multi_user_traffic_sample.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Initialize centralized forecaster\n",
    "    forecaster = CentralizedTrafficForecaster(df)\n",
    "\n",
    "    # Define training and evaluation periods\n",
    "    train_start = start_date  # previouse two month\n",
    "    train_end = start_date + timedelta(days=30)  # First month\n",
    "    eval_start = start_date + timedelta(days=35)  # A few days into second month\n",
    "    eval_end = start_date + timedelta(days=45)  # 10-day evaluation period\n",
    "\n",
    "    # Train the centralized model\n",
    "    print(f\"\\nTraining centralized model on data from {train_start.date()} to {train_end.date()}\")\n",
    "    training_losses = forecaster.train(\n",
    "        train_start,\n",
    "        train_end,\n",
    "        batch_size=64,\n",
    "        epochs=500  # Use more epochs (e.g., 50) for better results\n",
    "    )\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(training_losses)\n",
    "    plt.title('Centralized Model Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('centralized_model_training_loss.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Make predictions for a specific date\n",
    "    target_date = eval_start\n",
    "    print(f\"\\nGenerating predictions for all users on {target_date.date()}\")\n",
    "\n",
    "    # Plot predictions\n",
    "    fig = forecaster.plot_predictions(target_date)\n",
    "    plt.savefig('centralized_model_predictions.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate model on test period\n",
    "    print(f\"\\nEvaluating centralized model from {eval_start.date()} to {eval_end.date()}\")\n",
    "    evaluation_results = forecaster.evaluate(eval_start, eval_end)\n",
    "\n",
    "    # Print summary of evaluation results\n",
    "    if 'avg_metrics' in evaluation_results:\n",
    "        print(\"\\nEvaluation Summary:\")\n",
    "        for user_col, metrics in evaluation_results['avg_metrics'].items():\n",
    "            print(f\"{user_col}: RMSE={metrics['avg_rmse']:.2f}, MAE={metrics['avg_mae']:.2f}, MAPE={metrics['avg_mape']:.2f}%\")\n",
    "\n",
    "    # Plot evaluation metrics\n",
    "    fig = forecaster.plot_evaluation_metrics(evaluation_results)\n",
    "    plt.savefig('centralized_model_evaluation.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Save model\n",
    "    forecaster.save_model('centralized_traffic_forecaster.pt')\n",
    "\n",
    "    print(\"\\nCentralized traffic forecasting completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecaster.load_model(('centralized_traffic_forecaster.pt'))\n",
    "# model.eval()\n",
    "target_date = datetime(2025, 2, 17, 0, 0)\n",
    "user_predictions, target_hours = forecaster.predict(target_date)\n",
    "type(user_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(user_predictions, index=target_hours)\n",
    "print(df)\n",
    "df.to_excel(\"user_predictions.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### user_row, 24hour_column\n",
    "user_dic = user_predictions\n",
    "user_array = list(user_dic.values())\n",
    "all_user = np.stack(user_array)\n",
    "all_user[:,16] # all user in time 16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
